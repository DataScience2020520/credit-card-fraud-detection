{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Fraud Dection\n",
    "###### Procedure to process:\n",
    "1. load data\n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Datasets/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data[\"Class\"], sort = True).sort_index()\n",
    "ax = count_classes.plot(kind = \"bar\")\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "for value, p in zip(count_classes.values, count_classes.index): \n",
    "    plt.text(p-0.1, value + 2000,value, fontsize = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stardant Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data[\"normAmount\"] = StandardScaler().fit_transform(data[\"Amount\"].values.reshape(-1,1))\n",
    "data = data.drop(['Time', 'Amount'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of Normal smpale:  0.5\n",
      "Ratio of Fraud smpale:  0.5\n",
      "Total of under smpale data:  984\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, data.columns != \"Class\"]\n",
    "y = data.iloc[:, data.columns =='Class']\n",
    "\n",
    "# get the index for all fraud data records\n",
    "number_records_fraud = len(data[data.Class == 1])\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# get the index for all no fraud data records\n",
    "normal_indices = np.array(data[data.Class == 0].index)\n",
    "\n",
    "# get a random list of normal sample\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "#get all under_sample_data\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != \"Class\"]\n",
    "y_undersample = under_sample_data.iloc[:, under_sample_data.columns =='Class']\n",
    "\n",
    "#under sample data summary\n",
    "\n",
    "print(\"Ratio of Normal smpale: \", len(under_sample_data[under_sample_data.Class ==0])/len(under_sample_data))\n",
    "print(\"Ratio of Fraud smpale: \", len(under_sample_data[under_sample_data.Class ==1])/len(under_sample_data))\n",
    "print(\"Total of under smpale data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the histgram of under sample data\n",
    "count_classes = pd.value_counts(under_sample_data[\"Class\"], sort = True).sort_index()\n",
    "ax = count_classes.plot(kind = \"bar\")\n",
    "plt.title(\"Under Sample Data Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "for value, p in zip(count_classes.values, count_classes.index): \n",
    "    plt.text(p-0.1, value + 2,value, fontsize = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide Data For Cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of original training data:  227845\n",
      "The number of original testing data:  56962\n",
      "The total of original data 284807\n",
      "\n",
      "\n",
      "The number of undersample training data:  787\n",
      "The number of undersample testing data:  197\n",
      "The total of undersample data 984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "\n",
    "print(\"The number of original training data: \", len(X_train))\n",
    "print(\"The number of original testing data: \", len(X_test))\n",
    "print(\"The total of original data\", len(X_train) + len(X_test))\n",
    "\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, \n",
    "                                                                                                    y_undersample, test_size=0.2,random_state = 0)\n",
    "print(\"\\n\")\n",
    "print(\"The number of undersample training data: \", len(X_train_undersample))\n",
    "print(\"The number of undersample testing data: \", len(X_test_undersample))\n",
    "print(\"The total of undersample data\", len(X_train_undersample) + len(X_test_undersample))                                                                                                     \n",
    "                                                                                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "from sklearn.metrics import confusion_matrix, recall_score, classification_report\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_KFold_scores(x_train_data, y_train_data):\n",
    "    kf = KFold(5, random_state=None, shuffle=False)\n",
    "    fold = kf.split(x_train_data)\n",
    "    c_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns =['C_parameter', 'Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "   \n",
    "    # k-fold \n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        \n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"c_param_value\", c_param)\n",
    "        print(\"----------------------------------------\")\n",
    "        recall_accs = []\n",
    "        iteration = 1 \n",
    "        fold = kf.split(x_train_data)\n",
    "        for train_index, valid_index in fold:\n",
    "           lr = LogisticRegression (C=c_param, penalty = 'l2')\n",
    "           lr.fit(x_train_data.iloc[train_index,:], y_train_data.iloc[train_index,:].values.ravel())\n",
    "        \n",
    "           y_pred_undersample = lr.predict(x_train_data.iloc[valid_index,:].values)\n",
    "           recall_acc = recall_score(y_train_data.iloc[valid_index,:].values,y_pred_undersample)\n",
    "           \n",
    "           recall_accs.append(recall_acc) \n",
    "           print(\"Iteration\",iteration, \"recall:\", recall_acc ) \n",
    "           iteration = iteration +1 \n",
    "        \n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j +=1\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"mean recall\", np.mean(recall_accs))\n",
    "        print(\"\")\n",
    "        \n",
    "    best_c = results_table.loc[results_table[\"Mean recall score\"].astype('float64').idxmax()]['C_parameter']    \n",
    "   \n",
    "\n",
    "    print(\"*********************************\") \n",
    "    print(\"best c\", best_c)\n",
    "    print(\"*********************************\") \n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "c_param_value 0.001\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.7865168539325843\n",
      "Iteration 2 recall: 0.8214285714285714\n",
      "Iteration 3 recall: 0.7971014492753623\n",
      "Iteration 4 recall: 0.8658536585365854\n",
      "Iteration 5 recall: 0.8701298701298701\n",
      "\n",
      "mean recall 0.8282060806605946\n",
      "\n",
      "----------------------------------------\n",
      "c_param_value 0.01\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.8314606741573034\n",
      "Iteration 2 recall: 0.8452380952380952\n",
      "Iteration 3 recall: 0.855072463768116\n",
      "Iteration 4 recall: 0.9512195121951219\n",
      "Iteration 5 recall: 0.8961038961038961\n",
      "\n",
      "mean recall 0.8758189282925064\n",
      "\n",
      "----------------------------------------\n",
      "c_param_value 0.1\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.8876404494382022\n",
      "Iteration 2 recall: 0.8690476190476191\n",
      "Iteration 3 recall: 0.9130434782608695\n",
      "Iteration 4 recall: 0.9634146341463414\n",
      "Iteration 5 recall: 0.8961038961038961\n",
      "\n",
      "mean recall 0.9058500153993856\n",
      "\n",
      "----------------------------------------\n",
      "c_param_value 1\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.8876404494382022\n",
      "Iteration 2 recall: 0.8809523809523809\n",
      "Iteration 3 recall: 0.9565217391304348\n",
      "Iteration 4 recall: 0.9634146341463414\n",
      "Iteration 5 recall: 0.9090909090909091\n",
      "\n",
      "mean recall 0.9195240225516537\n",
      "\n",
      "----------------------------------------\n",
      "c_param_value 10\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.898876404494382\n",
      "Iteration 2 recall: 0.8809523809523809\n",
      "Iteration 3 recall: 0.9565217391304348\n",
      "Iteration 4 recall: 0.9634146341463414\n",
      "Iteration 5 recall: 0.9090909090909091\n",
      "\n",
      "mean recall 0.9217712135628895\n",
      "\n",
      "----------------------------------------\n",
      "c_param_value 100\n",
      "----------------------------------------\n",
      "Iteration 1 recall: 0.9101123595505618\n",
      "Iteration 2 recall: 0.8809523809523809\n",
      "Iteration 3 recall: 0.9565217391304348\n",
      "Iteration 4 recall: 0.9634146341463414\n",
      "Iteration 5 recall: 0.9090909090909091\n",
      "\n",
      "mean recall 0.9240184045741255\n",
      "\n",
      "*********************************\n",
      "best c 100.0\n",
      "*********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/p0z00cx/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "best_c = printing_KFold_scores(X_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title = \"confuction matrix\", cmap = plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation ='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes, rotation=0)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    thresh = cm.max()/2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i, cm[i,j], horizontalalignment ='center', color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with undersample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l2')\n",
    "lr.fit(X_train_undersample, y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test_undersample, y_pred_undersample)\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "print(\"precision\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))\n",
    "print(\"recall\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Cofusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l2')\n",
    "lr.fit(X_train_undersample, y_train_undersample.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "print(\"precision\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))\n",
    "print(\"recall\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Cofusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n",
    "print(pd.DataFrame(y_pred_undersample_proba[:,1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate the effec of the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best_c got from the above experiments\n",
    "lr = LogisticRegression(C=10.0, penalty ='l2')\n",
    "\n",
    "lr.fit(X_train_undersample, y_train_undersample.values.ravel())\n",
    "\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "j = 1\n",
    "\n",
    "for i in thresholds:\n",
    "    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j)\n",
    "    j =j+1\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test_undersample, y_test_predictions_high_recall)\n",
    "    np.set_printoptions(precision = 2)\n",
    "    \n",
    "    print(\"threshold value:\", i, \"recall value: \", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "    \n",
    "    class_names = [0, 1]\n",
    "    plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Threshold >=%s\"%i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best_c got from the above experiments\n",
    "lr = LogisticRegression(C=10.0, penalty ='l2')\n",
    "\n",
    "lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test.values)\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "j = 1\n",
    "\n",
    "for i in thresholds:\n",
    "    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j)\n",
    "    j =j+1\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, y_test_predictions_high_recall)\n",
    "    np.set_printoptions(precision = 2)\n",
    "    \n",
    "    print(\"threshold value:\", i, \"recall value: \", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "    print(\"threshold value:\", i, \"precision value: \",  cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))\n",
    "    class_names = [0, 1]\n",
    "    plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Threshold >=%s\"%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample - SMOTE solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.copy()\n",
    "labels = y.copy()\n",
    "\n",
    "features_train, features_test,labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state = 0)\n",
    "\n",
    "print(\"The number of original training data: \", len(features_train))\n",
    "print(\"The number of original testing data: \", len(features_test))\n",
    "print(\"The total of original data\", len(features_train) + len(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = SMOTE(random_state = 0)\n",
    "os_features, os_labels = oversampler.fit_sample(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the histgram of under sample data\n",
    "count_classes = pd.value_counts(os_labels[\"Class\"], sort = True).sort_index()\n",
    "ax = count_classes.plot(kind = \"bar\")\n",
    "plt.title(\"Over Sample Data Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "for value, p in zip(count_classes.values, count_classes.index): \n",
    "    plt.text(p-0.1, value + 2,value, fontsize = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_KFold_scores(os_features, os_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l2')\n",
    "lr.fit(os_features, os_labels.values.ravel())\n",
    "y_pred_oversample = lr.predict(os_features.values)\n",
    "\n",
    "cnf_matrix = confusion_matrix(os_labels, y_pred_oversample)\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "print(\"precision\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))\n",
    "print(\"recall\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Cofusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l2')\n",
    "lr.fit(os_features, os_labels.values.ravel())\n",
    "y_pred_oversample = lr.predict(features_test.values)\n",
    "\n",
    "cnf_matrix = confusion_matrix(labels_test, y_pred_oversample)\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "print(\"precision\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1]))\n",
    "print(\"recall\", cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = class_names, title =\"Cofusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "1. How to deal with unbalanced datea under sample and over sample \n",
    "2. preprocesing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
